{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 5\n",
    "\n",
    "This notebook exposes the code to reproduce the figure 5 of the paper, that represent systematic evaluation of all the models considered in the paper. It may take a while (usually a dozen of hours on a laptop). Indeed, it trains approximately 4000 neural networks.\n",
    "\n",
    "However, the data to create this figure have been added in the github repository. If you want to simply reproduce the figure, you can check the next notebook [2_Figure5_part2](2_Figure5_part2.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Input\n",
    "from tensorflow.keras.layers import add as k_add\n",
    "from tensorflow.keras.layers import multiply as k_multiply\n",
    "from tensorflow.keras.layers import concatenate as k_concatenate\n",
    "\n",
    "import scipy\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System definition\n",
    "def S(x, tau):\n",
    "    res = np.exp(-x**2) - 0.1*tau*(np.abs(x)-4.)  \n",
    "    return res\n",
    "# data generating process\n",
    "# Generate Train set\n",
    "def generate_dataset(p=0.01, N_sample=1000):\n",
    "    X = np.random.uniform(-4,4,N_sample)\n",
    "    TAU = np.random.binomial(1, p, size=N_sample)\n",
    "    XTAU = np.c_[X,TAU]\n",
    "    Y = S(X,TAU)\n",
    "    return XTAU, Y, X, TAU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(nE = 1, nD = 1, ls = 20, enc = \"FC\", nltype=\"relu\"):\n",
    "    \"\"\"\n",
    "    :param nE: number of layers for encoder E\n",
    "    :param nD: number of layers for decoder D\n",
    "    :param ls: size of each layer\n",
    "    :param enc: one of \"FC\" / \"LEAP\" the model considered\n",
    "    \"\"\"\n",
    "    x = Input(shape=(1,))\n",
    "    tau = Input(shape=(1,))\n",
    "    \n",
    "    # build the input of the model\n",
    "    if enc == \"FC\" or enc == \"ResNet\" :\n",
    "        x_ = k_concatenate([x, tau])  # in the fully connected, the neural network uses a concantenation of x and tau\n",
    "    elif enc == \"LEAP\":\n",
    "        x_ = x\n",
    "    else:\n",
    "        raise RuntimeError(\"Unknown encoding \\\"{}\\\"\".format(enc))\n",
    "    tmp = x_\n",
    "\n",
    "    # enoder (denoted by E)\n",
    "    for _ in range(nE):\n",
    "        tmp = Dense(ls)(tmp)\n",
    "        tmp = Activation(nltype)(tmp)\n",
    "    E_out = tmp\n",
    "\n",
    "    # leap part (L_tau)\n",
    "    if enc == \"LEAP\":\n",
    "        tmp = Dense(1)(E_out) # e\n",
    "        tmp = k_multiply([tau, tmp])  # element wise multiplication\n",
    "        tmp = Dense(ls, use_bias=False)(tmp) # d\n",
    "        d_in = k_add([E_out, tmp])\n",
    "    elif enc == \"FC\":\n",
    "        d_in = E_out\n",
    "    elif enc == \"ResNet\":\n",
    "        tmp = Dense(1)(E_out)\n",
    "        tmp = Activation(nltype)(tmp)\n",
    "        tmp = Dense(ls)(tmp)\n",
    "        d_in = k_add([E_out, tmp])\n",
    "    else:\n",
    "        raise RuntimeError(\"Unknown encoding \\\"{}\\\"\".format(enc))\n",
    "    \n",
    "    # decoder (denoted by D)\n",
    "    tmp = d_in\n",
    "    for _ in range(nD):\n",
    "        tmp = Dense(ls)(tmp)\n",
    "        tmp = Activation(nltype)(tmp)\n",
    "    D_out = tmp\n",
    "    \n",
    "    # last layer\n",
    "    res_model = Dense(1)(tmp)\n",
    "\n",
    "\n",
    "    res = Model(inputs=[x, tau], outputs=[res_model])\n",
    "    return res\n",
    "\n",
    "def reset_weights(model):\n",
    "    session = keras.backend.get_session()\n",
    "    for layer in model.layers: \n",
    "        if hasattr(layer, 'kernel_initializer'):\n",
    "            layer.kernel.initializer.run(session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer parameter\n",
    "lr = 0.001\n",
    "max_iter = 2000\n",
    "batch_size=32\n",
    "\n",
    "# models parameters\n",
    "nE = 5\n",
    "nD = 1\n",
    "ls = 10\n",
    "\n",
    "# Data parameters\n",
    "N_sample = 128\n",
    "p_test = 0.5\n",
    "N_per_p = 100\n",
    "\n",
    "## test set\n",
    "X_TAU_test, Y_test, X_test, TAU_test = generate_dataset(p=p_test, N_sample=N_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "132eba5619064d81a65be3ce1f99cc09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe93c764c944a158d08bd442b1b1110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = {}\n",
    "scores_minus1 = {}\n",
    "scores_plus1 = {}\n",
    "scores_minus1_train = {}\n",
    "scores_plus1_train = {}\n",
    "p_trains = []\n",
    "\n",
    "scores[\"leap\"] = {}\n",
    "scores[\"fc\"] = {}\n",
    "scores[\"os\"] = {}\n",
    "scores[\"rn\"] = {}\n",
    "scores_minus1[\"leap\"] = {}\n",
    "scores_minus1[\"fc\"] = {}\n",
    "scores_minus1[\"os\"] = {}\n",
    "scores_minus1[\"rn\"] = {}\n",
    "scores_plus1[\"leap\"] = {}\n",
    "scores_plus1[\"fc\"] = {}\n",
    "scores_plus1[\"os\"] = {}\n",
    "scores_plus1[\"rn\"] = {}\n",
    "scores_minus1_train[\"leap\"] = {}\n",
    "scores_minus1_train[\"fc\"] = {}\n",
    "scores_minus1_train[\"os\"] = {}\n",
    "scores_minus1_train[\"rn\"] = {}\n",
    "scores_plus1_train[\"leap\"] = {}\n",
    "scores_plus1_train[\"fc\"] = {}\n",
    "scores_plus1_train[\"os\"] = {}\n",
    "scores_plus1_train[\"rn\"] = {}\n",
    "\n",
    "\n",
    "for sample in tqdm(range(N_per_p)):    \n",
    "    for p_gen in tqdm(range(0,10)):\n",
    "        p_train = np.exp(-p_gen*0.5)/(1+np.exp(-p_gen*0.5))\n",
    "        \n",
    "        if not p_train in scores[\"leap\"]:\n",
    "            scores[\"leap\"][p_train] = []\n",
    "            scores[\"fc\"][p_train] = []\n",
    "            scores[\"os\"][p_train] = []\n",
    "            scores[\"rn\"][p_train] = []\n",
    "            \n",
    "            scores_minus1[\"leap\"][p_train] = []\n",
    "            scores_plus1[\"leap\"][p_train] = []\n",
    "            scores_minus1[\"fc\"][p_train] = []\n",
    "            scores_plus1[\"fc\"][p_train] = []\n",
    "            scores_minus1[\"os\"][p_train] = []\n",
    "            scores_plus1[\"os\"][p_train] = []\n",
    "            scores_minus1[\"rn\"][p_train] = []\n",
    "            scores_plus1[\"rn\"][p_train] = []\n",
    "            \n",
    "            scores_minus1_train[\"leap\"][p_train] = []\n",
    "            scores_plus1_train[\"leap\"][p_train] = []\n",
    "            scores_minus1_train[\"fc\"][p_train] = []\n",
    "            scores_plus1_train[\"fc\"][p_train] = []\n",
    "            scores_minus1_train[\"os\"][p_train] = []\n",
    "            scores_plus1_train[\"os\"][p_train] = []\n",
    "            scores_minus1_train[\"rn\"][p_train] = []\n",
    "            scores_plus1_train[\"rn\"][p_train] = []\n",
    "            \n",
    "        # generate a new training dataset\n",
    "        X_TAU_train, Y_train, X_train, TAU_train = generate_dataset(p=p_train, N_sample=N_sample)\n",
    "        \n",
    "        # build models\n",
    "        adam_leap = tf.optimizers.Adam(lr=lr)\n",
    "        leap_model = build_model(nE=nE, nD=nD, ls=ls, enc=\"LEAP\")\n",
    "        leap_model.compile(optimizer=adam_leap, loss='mse')\n",
    "        leap_model.fit(x=[X_train, TAU_train], y=[Y_train], epochs=max_iter, verbose=False, batch_size=batch_size)\n",
    "        Y_pred_leap = leap_model.predict([X_test, TAU_test])\n",
    "        Y_pred_leap_train = leap_model.predict([X_train, TAU_train])\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        adam_fc = tf.optimizers.Adam(lr=lr)\n",
    "        fc_model = build_model(nE=nE, nD=nD, ls=ls, enc=\"FC\")\n",
    "        fc_model.compile(optimizer=adam_fc, loss='mse')\n",
    "        fc_model.fit(x=[X_train, TAU_train], y=[Y_train], epochs=max_iter, verbose=False, batch_size=batch_size)\n",
    "        Y_pred_fc = fc_model.predict([X_test, TAU_test])\n",
    "        Y_pred_fc_train = fc_model.predict([X_train, TAU_train])\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        # ResNet fully connected\n",
    "        adam_rn = tf.optimizers.Adam(lr=lr)\n",
    "        rn_model = build_model(nE=nE, nD=nD, ls=ls, enc=\"ResNet\")\n",
    "        rn_model.compile(optimizer=adam_rn, loss='mse')\n",
    "        rn_model.fit(x=[X_train, TAU_train], y=[Y_train], epochs=max_iter, verbose=False, batch_size=batch_size)\n",
    "        Y_pred_rn = rn_model.predict([X_test, TAU_test])\n",
    "        Y_pred_rn_train = rn_model.predict([X_train, TAU_train])\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        # over sampling the cases the less freqent\n",
    "        adam_os = tf.optimizers.Adam(lr=lr)\n",
    "        os_model = build_model(nE=nE, nD=nD, ls=ls, enc=\"ResNet\")\n",
    "        os_model.compile(optimizer=adam_os, loss='mse')\n",
    "        weights = np.ones(TAU_train.shape, dtype=np.float32)\n",
    "        weights[TAU_train == 0] = np.float32(1. / (1.-p_train))\n",
    "        weights[TAU_train == 1] = np.float32(1. / (p_train))\n",
    "        weights = weights.flatten()\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            # buggy tensorflow 2.1 warning\n",
    "            # https://github.com/tensorflow/tensorflow/issues/37500\n",
    "            os_model.fit(x=[X_train, TAU_train],\n",
    "                         y=[Y_train],\n",
    "                         epochs=max_iter,\n",
    "                         verbose=False,\n",
    "                         batch_size=batch_size,\n",
    "                         sample_weight=weights)\n",
    "        Y_pred_os = os_model.predict([X_test, TAU_test])\n",
    "        Y_pred_os_train = os_model.predict([X_train, TAU_train])\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        scores[\"leap\"][p_train].append(mean_squared_error(Y_test, Y_pred_leap))\n",
    "        scores_minus1[\"leap\"][p_train].append(mean_squared_error(Y_test[TAU_test==0], Y_pred_leap[TAU_test==0]))\n",
    "        scores_plus1[\"leap\"][p_train].append(mean_squared_error(Y_test[TAU_test==1], Y_pred_leap[TAU_test==1]))\n",
    " \n",
    "        scores[\"fc\"][p_train].append(mean_squared_error(Y_test, Y_pred_fc))\n",
    "        scores_minus1[\"fc\"][p_train].append(mean_squared_error(Y_test[TAU_test==0], Y_pred_fc[TAU_test==0]))\n",
    "        scores_plus1[\"fc\"][p_train].append(mean_squared_error(Y_test[TAU_test==1], Y_pred_fc[TAU_test==1]))\n",
    " \n",
    "        scores[\"os\"][p_train].append(mean_squared_error(Y_test, Y_pred_os))\n",
    "        scores_minus1[\"os\"][p_train].append(mean_squared_error(Y_test[TAU_test==0], Y_pred_os[TAU_test==0]))\n",
    "        scores_plus1[\"os\"][p_train].append(mean_squared_error(Y_test[TAU_test==1], Y_pred_os[TAU_test==1]))\n",
    " \n",
    "        scores[\"rn\"][p_train].append(mean_squared_error(Y_test, Y_pred_rn))\n",
    "        scores_minus1[\"rn\"][p_train].append(mean_squared_error(Y_test[TAU_test==0], Y_pred_rn[TAU_test==0]))\n",
    "        scores_plus1[\"rn\"][p_train].append(mean_squared_error(Y_test[TAU_test==1], Y_pred_rn[TAU_test==1]))\n",
    "        \n",
    "        scores_minus1_train[\"fc\"][p_train].append(mean_squared_error(Y_train[TAU_train==0],\n",
    "                                                                     Y_pred_fc_train[TAU_train==0]))\n",
    "        scores_minus1_train[\"os\"][p_train].append(mean_squared_error(Y_train[TAU_train==0],\n",
    "                                                                       Y_pred_os_train[TAU_train==0]))\n",
    "        scores_minus1_train[\"leap\"][p_train].append(mean_squared_error(Y_train[TAU_train==0],\n",
    "                                                                       Y_pred_leap_train[TAU_train==0]))\n",
    "        scores_minus1_train[\"rn\"][p_train].append(mean_squared_error(Y_train[TAU_train==0],\n",
    "                                                                       Y_pred_rn_train[TAU_train==0]))\n",
    "        \n",
    "        if np.sum(TAU_train==1) >= 1:\n",
    "            scores_plus1_train[\"fc\"][p_train].append(mean_squared_error(Y_train[TAU_train==1],\n",
    "                                                                        Y_pred_fc_train[TAU_train==1]))\n",
    "            scores_plus1_train[\"leap\"][p_train].append(mean_squared_error(Y_train[TAU_train==1],\n",
    "                                                                          Y_pred_leap_train[TAU_train==1]))\n",
    "            scores_plus1_train[\"os\"][p_train].append(mean_squared_error(Y_train[TAU_train==1],\n",
    "                                                                          Y_pred_os_train[TAU_train==1]))\n",
    "            scores_plus1_train[\"rn\"][p_train].append(mean_squared_error(Y_train[TAU_train==1],\n",
    "                                                                          Y_pred_rn_train[TAU_train==1]))\n",
    "        else:\n",
    "            scores_plus1_train[\"fc\"][p_train].append(-1)\n",
    "            scores_plus1_train[\"leap\"][p_train].append(-1)\n",
    "            scores_plus1_train[\"os\"][p_train].append(-1)\n",
    "            scores_plus1_train[\"rn\"][p_train].append(-1)\n",
    "            \n",
    "        with open(\"scores_5.json\", \"w\") as f:\n",
    "            json.dump(scores, f, indent=4, sort_keys=True)\n",
    "        with open(\"scores_minus1_5.json\", \"w\") as f:\n",
    "            json.dump(scores_minus1, f, indent=4, sort_keys=True)\n",
    "        with open(\"scores_plus1_5.json\", \"w\") as f:\n",
    "            json.dump(scores_plus1, f, indent=4, sort_keys=True)\n",
    "        with open(\"scores_minus1_train_5.json\", \"w\") as f:\n",
    "            json.dump(scores_minus1_train, f, indent=4, sort_keys=True)\n",
    "        with open(\"scores_plus1_train_5.json\", \"w\") as f:\n",
    "            json.dump(scores_plus1_train, f, indent=4, sort_keys=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
